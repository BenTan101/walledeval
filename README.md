# bastion
> _Test LLMs against jailbreaks and unprecedented harms_

Bastion is a simple library to test LLM safety by identifying if text generated by the LLM is indeed safe. We purposefully test benchmarks with negative information and toxic prompts to see if it is able to flag prompts of malice.

## Basic Usage

### LLMs (`bastion.llm`)
We support the following LLM types:

|Class|LLM Type|
|---|---|
|`HF_LLM(id, system_prompt = "")`|Any HuggingFace LLM that supports Text Generation, specified with `id` parameter.|
|`Claude(api_key, system_prompt = "")`|Claude 3 Opus|


Usage is as follows:
```python
>>> from bastion.llm import HF_LLM, Claude

>>> hf_llm = HF_LLM("<insert llm identifier>")
>>> hf_llm.generate("How are you?")
# <output>

>>> claude = Claude("INSERT_API_KEY")
>>> claude.generate("How are you?")
# <output>
```

A custom abstract `LLM` class is also defined to support other LLMs, which takes in the model identifier `name` and optional system prompt `system_prompt`, and an abstract method `generate(text: str) -> str`.


### Guards (`bastion.guard`)
Guards are used to identify if outputs are malignant. We currently support the guard `ClaudeGuard`, which uses Claude 3 Opus and a custom-defined taxonomy to test malignant outputs. It returns `False` if malignant (i.e. it didn't pass the test).

Usage is as follows:
```python
>>> from bastion.guard import ClaudeGuard

>>> guard = ClaudeGuard("INSERT_API_KEY")
>>> guard.check("<insert output>")
# <boolean output>
```

A custom abstract `Guard` class is also defined to support other possible guards, which takes in the guard identifier `name`, and an abstract method `check(text: str) -> bool`.


### Evaluation (`bastion.eval`)
Benchmarks are available to provide datasets to test both the LLM and Guards. We currently test the following benchmarks:

|Benchmark Name|Class|
|---|---|
|[WMDP Benchmark](https://www.wmdp.ai/)|`WMDP`|

Usage is as follows:
```python
>>> from bastion.eval import WMDP

>>> wmdp = WMDP()

>>> wmdp.test(llm, guard)
# <logs>
# generator[logs]
```

A custom abstract `Benchmark` class is also defined for you to define your own benchmarks. We recommend reading the codebase to understand the general flow of WMDP.