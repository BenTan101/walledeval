# walledeval

> _Test LLMs against jailbreaks and unprecedented harms_

<!-- [![Python Package tests status](https://github.com/three-body-analysis/tris/actions/workflows/python-package.yml/badge.svg)](https://github.com/three-body-analysis/tris/actions?query=workflow%3Apython-package) -->
<!-- [![Docs CI status](https://github.com/three-body-analysis/tris/actions/workflows/docs.yml/badge.svg)](https://three-body-analysis.github.io/tris/) -->
[![PyPI Latest Release](https://img.shields.io/pypi/v/walledeval.svg)](https://pypi.org/project/walledeval/)
[![PyPI Downloads](https://static.pepy.tech/badge/walledeval)](https://pepy.tech/project/walledeval)

WalledEval is a simple library to test LLM safety by identifying if text generated by the LLM is indeed safe. We purposefully test benchmarks with negative information and toxic prompts to see if it is able to flag prompts of malice.

> [!NOTE]  
> We have recently released `v0.1.0` of our codebase! This means that our documentation is not completely up-to-date with the current state of the codebase. However, we will be updating our documentation soon for all users to be able to quickstart using WalledEval! Till then, it is always best to consult the code or the `tests/` or `notebooks/` folders to have a better idea of how the codebase currently works.

## Installation

### Installing from PyPI

Yes, we have published WalledEval on PyPI! To install WalledEval and all its dependencies, the easiest method would be to use 
`pip` to query PyPI. This should, by default, be present in your Python installation. To, install run the following 
command in a terminal or Command Prompt / Powershell:

```bash
$ pip install walledeval
```

Depending on the OS, you might need to use `pip3` instead. If the command is not found, you can choose to use the
following command too:

```bash
$ python -m pip install walledeval
```

Here too, `python` or `pip` might be replaced with `py` or `python3` and `pip3` depending on the OS and installation 
configuration. If you have any issues with this, it is always helpful to consult 
[Stack Overflow](https://stackoverflow.com/).

### Installing from Source

To install from source, you need to get the following:

#### Git

Git is needed to install this repository. This is not completely necessary as you can also install the zip file for this 
repository and store it on a local drive manually. To install Git, follow 
[this guide](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).

After you have successfully installed Git, you can run the following command in a terminal / Command Prompt etc:

```bash
$ git clone https://github.com/walledai/walledeval.git
```

This stores a copy in the folder `walledeval`. You can then navigate into it using `cd walledeval`.

#### Poetry

This project can be used easily via a tool known as Poetry. This allows you to easily reflect edits made in the original 
source code! To install `poetry`, you can also install it using `pip` by typing in the command as follows:

```bash
$ pip install poetry
```

Again, if you have any issues with `pip`, check out [here](#installing-from-pypi).

After this, you can use the following command to install this library:

```bash
$ poetry install
```


## Basic Usage

### LLMs (`walledeval.llm`)

We support the following LLM types:


| Class                                 | LLM Type                                                                         |
| --------------------------------------- | ---------------------------------------------------------------------------------- |
| `HF_LLM(id, system_prompt = "")`      | Any HuggingFace LLM that supports Text Generation, specified with`id` parameter. |
| `Claude(api_key, system_prompt = "")` | Claude 3 Opus                                                                    |

Usage is as follows:

```python
>>> from walledeval.llm import HF_LLM, Claude

>>> hf_llm = HF_LLM("<insert llm identifier>")
>>> hf_llm.generate("How are you?")
# <output>

>>> claude = Claude("INSERT_API_KEY")
>>> claude.generate("How are you?")
# <output>
```

A custom abstract `llm.LLM` class is also defined to support other LLMs, which takes in the model identifier `name` and optional system prompt `system_prompt`, and an abstract method `generate(text: str) -> str`.

### Judges (`walledeval.judge`)

Judges are used to identify if outputs are malignant. We currently support the judge `ClaudeJudge`, which uses Claude 3 Opus and a custom-defined taxonomy to test malignant outputs. It returns `False` if malignant (i.e. it didn't pass the test).

Usage is as follows:

```python
>>> from walledeval.judge import ClaudeJudge

>>> judge = ClaudeJudge("INSERT_API_KEY")
>>> judge.check("<insert output>")
# <boolean output>
```

A custom abstract `judge.Judge` class is also defined to support other possible judges, which takes in the judge identifier `name`, and an abstract method `check(text: str) -> bool`.

### Benchmarks (`walledeval.benchmark`)

Benchmarks are available to provide datasets to test both the LLM and Judges. We currently test the following benchmarks:


| Benchmark Name                         | Class  |
| ---------------------------------------- | -------- |
| [WMDP Benchmark](https://www.wmdp.ai/) | `WMDP` |

Usage is as follows:

```python
>>> from walledeval.benchmark import WMDP

>>> wmdp = WMDP()

>>> wmdp.test(llm, judge)
# <logs>
# generator[logs]
```

A custom abstract `benchmark.Benchmark` class is also defined for you to define your own benchmarks. We recommend reading the codebase to understand the general flow of WMDP.
